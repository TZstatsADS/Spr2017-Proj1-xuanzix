ggplot(america,aes(x = name, y = count, group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(x = "Presidents", y = "Number of America") +
theme.widebar
f.plotsent.len=function(In.list, InFile, InType, InTerm, President){
col.use <- c("blue", "red")
In.list$topemotion=apply(select(In.list,
negative:positive),
1, which.max)
In.list$topemotion.v=apply(select(In.list,
negative:positive),
1, max)
temp <- In.list$topemotion.v
In.list$topemotion.v[In.list$topemotion==1] <- -temp
#In.list$topemotion.v[abs(In.list$topemotion.v)<0.05]=0
df <- In.list%>%filter(File==InFile,
type==InType, Term==InTerm)%>%
select(sent.id, topemotion, topemotion.v)
dct_values <- get_transformed_values(
df$topemotion.v,
low_pass_size = 5,
x_reverse_len = 100,
scale_vals = F,
scale_range = T
)
dct_signs <- 0.5*sign(dct_values)+1.5
ptcol.use <- col.use[dct_signs]
plot(dct_values,
col=ptcol.use,
lwd="2",
type="h",
main=President, ylab = "level", xlab = "Transcript")
abline(h=0,lwd="2")
}
par(mfrow=c(1,2))
f.plotsent.len(In.list=sentence.list, InFile="DonaldJTrump",
InType="inaug", InTerm=1, President="Donald J. Trump")
f.plotsent.len(In.list=sentence.list, InFile="BarackObama",
InType="inaug", InTerm=1, President="Barack Obama")
knitr::include_graphics("/Users/xuxuanzi/Desktop/Spr2017-Proj1-xuanzix/figs/2.jpg")
par(mfrow=c(1,2))
f.plotsent.len(In.list=sentence.list, InFile="GeorgeWBush",
InType="inaug", InTerm=1, President="George W. Bush")
f.plotsent.len(In.list=sentence.list, InFile="WilliamJClinton",
InType="inaug", InTerm=1, President="William J. Clinton")
get(wd)
getwd()
speech.list$fulltext<-NA
for(i in seq(nrow(speech.list))) {
text <- read_html(speech.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speech.list$fulltext[i]=text}
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/1.png")
knitr::include_graphics(".../figs/2.jpg")
getwd()
?knitr::knit
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
setwd("~/Desktop/Spr2017-Proj1-xuanzix/doc")
inaug.list<-read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
View(inaug.list)
speech.list<-inaug.list
speech.list$type<-rep("inaug", nrow(inaug.list))
View(speech.list)
colnames(speech.list)[1]<-"President"
folder.path=".../data/fulltext/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path,encoding="UTF-8"))
getwd()
folder.path="/Users/xuxuanzi/Desktop/Spr2017-Proj1-xuanzix/data/fulltext/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path,encoding="UTF-8"))
ff.all[[1]]
View(ff.all[[1]])
as.data.frame(ff.all)
a<-as.data.frame(ff.all)
View(a)
seq(nrow(speech.list))
speech.list$fulltext<-NA
for(i in seq(nrow(speech.list))) {
text <- a$text[i] %>% # load the page
speech.list$fulltext[i]<-text}
speech.list$fulltext<-NA
for(i in seq(nrow(speech.list))) {
text <- a$text[i]
speech.list$fulltext[i]<-text}
View(speech.list)
speech.list$fulltext[1]
prez.out
?arrange
speech.list<-arrange(speech.list, President, Term)
View(speech.list)
colnames(speech.list)[1]<-"President"
folder.path="/Users/xuxuanzi/Desktop/Spr2017-Proj1-xuanzix/data/fulltext/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path,encoding="UTF-8"))
a<-as.data.frame(ff.all)
speech.list$fulltext<-NA
for(i in seq(nrow(speech.list))) {
text <- a$text[i]
speech.list$fulltext[i]<-text}
View(speech.list)
View(inaug.list)
sentence.list<-NULL
for(i in 1:nrow(speech.list)){
sentences<-sent_detect(speech.list$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions<-get_nrc_sentiment(sentences)
word.count<-word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list<-rbind(sentence.list,
cbind(speech.list[i,-ncol(speech.list)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list<-
sentence.list%>%
filter(!is.na(word.count))
View(inaug.list)
sentence.list.sel<-filter(sentence.list,
type=="inaug", Term==1)
sentence.list.sel$File<-factor(sentence.list.sel$File)
number_of_sentences<-tapply(sentence.list.sel$sentences, sentence.list.sel$President, length)# number of sentences
average_words_per_sentence<-round(tapply(sentence.list.sel$word.count, sentence.list.sel$President, mean),0) # average words per sentence
total_words<-tapply(sentence.list.sel$word.count, sentence.list.sel$President, sum)
count<-data.frame(number_of_sentences, average_words_per_sentence, total_words)
filenames <- Sys.glob(".../data/fulltext/*-1.txt")
texts <- filenames %>%
lapply(readLines) %>%
lapply(paste0, collapse = " ") %>%
lapply(as.String)
names(texts) <- substr(basename(filenames),6,nchar(basename(filenames))-4)
annotate_entities <- function(doc, annotation_pipeline) {
annotations <-  NLP::annotate(doc, annotation_pipeline)
AnnotatedPlainTextDocument(doc, annotations)
}
itinerants_pipeline <-list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator())
texts_annotated <- lapply(texts, annotate_entities, itinerants_pipeline)
#class(texts_annotated)
words<-lapply(texts_annotated, words)
#class(words)
count$vocabulary<-words%>%lapply(unique) %>%sapply(length)
getwd()
sentence.list.sel<-filter(sentence.list,
type=="inaug", Term==1)
sentence.list.sel$File<-factor(sentence.list.sel$File)
number_of_sentences<-tapply(sentence.list.sel$sentences, sentence.list.sel$President, length)# number of sentences
average_words_per_sentence<-round(tapply(sentence.list.sel$word.count, sentence.list.sel$President, mean),0) # average words per sentence
total_words<-tapply(sentence.list.sel$word.count, sentence.list.sel$President, sum)
count<-data.frame(number_of_sentences, average_words_per_sentence, total_words)
filenames <- Sys.glob("/Users/xuxuanzi/Desktop/Spr2017-Proj1-xuanzix/data/fulltext/*-1.txt")
texts <- filenames %>%
lapply(readLines) %>%
lapply(paste0, collapse = " ") %>%
lapply(as.String)
names(texts) <- substr(basename(filenames),6,nchar(basename(filenames))-4)
annotate_entities <- function(doc, annotation_pipeline) {
annotations <-  NLP::annotate(doc, annotation_pipeline)
AnnotatedPlainTextDocument(doc, annotations)
}
itinerants_pipeline <-list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator())
texts_annotated <- lapply(texts, annotate_entities, itinerants_pipeline)
#class(texts_annotated)
words<-lapply(texts_annotated, words)
#class(words)
count$vocabulary<-words%>%lapply(unique) %>%sapply(length)
count$total_words[3]<-2395;count$average_words_per_sentence[3]<-19
count$total_words[23]<-1366; count$average_words_per_sentence[23]<-24
count$total_words[28]<-2427; count$average_words_per_sentence[28]<-17
count$total_words[7]<-2459; count$average_words_per_sentence[7]<-18
colnames(count)<-c("Number of Sentences", "Average Words Per Sentence","Total Words","Total Vocabulary")
Average<-c(mean(count$`Number of Sentences`),mean(count$`Average Words Per Sentence`),mean(count$`Total Words`),mean(count$`Total Vocabulary`))
Average<-round(Average,0)
names(Average)<-colnames(count)
count<-rbind(Average,count)
rownames(count)[1]<-"average"
president.order<-c("average",rev(filter(inaug.list, Term==1)$inaug.list$X...President))
count1<-data.frame()
for(i in 1:40){
a<-count[rownames(count) == president.order[i],]
count1<-rbind(count1,a)
}
datatable(count1, options = list(pageLength = 10))
knitr::include_graphics(".../figs/3.jpg")
# Step 0: check and install needed packages. Load the libraries and functions.
packages.used=c("rvest",
"sentimentr", "qdap","gplots", "dplyr",
"tm", "syuzhet", "scales", "RColorBrewer",
"RANN", "tm","DT","languageR","NLP","openNLP","magrittr","ggplot2","tidytext","wordcloud","grid","gridExtra","knitr")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("scales")
library("RColorBrewer")
library("RANN")
library("DT")
library("languageR")
library("NLP")
library("openNLP")
library("magrittr")
library("ggplot2")
library("tidytext")
library("wordcloud")
library("grid")
library("gridExtra")
library("knitr")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.
#Following the example of [Jerid Francom](http://francojc.github.io/web-scraping-with-rvest/), we used [Selectorgadget](http://selectorgadget.com/) to choose the links we would like to scrap. For this project, we selected all inaugural addresses of past presidents.
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug<-f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug<-inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
# Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap.
inaug.list<-read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
#We assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts.
# Step 3: scrap the texts of speeches from the speech URLs.
speech.list<-inaug.list
speech.list$type<-rep("inaug", nrow(inaug.list))
speech.url<-inaug
speech.list<-cbind(speech.list, speech.url)
colnames(speech.list)[1]<-"President"
#Based on the list of speeches, we scrap the main text part of the transcript's html page. For simple html pages of this kind,  [Selectorgadget](http://selectorgadget.com/) is very convenient for identifying the html node that `rvest` can use to scrap its content. For reproducibility, we also save our scrapped speeches into our local folder as individual speech files.
# Loop over each row in speech.list
speech.list$fulltext<-NA
for(i in seq(nrow(speech.list))) {
text <- read_html(speech.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speech.list$fulltext[i]=text}
# Create the file name
#  filename <- paste0("../data/fulltext/",
#                     speech.list$type[i],
#                     speech.list$File[i], "-",
#                     speech.list$Term[i], ".txt")
#  sink(file = filename) %>% # open file to write
#  cat(text)  # write the file
#  sink() # close the file
#}
sentence.list<-NULL
for(i in 1:nrow(speech.list)){
sentences<-sent_detect(speech.list$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions<-get_nrc_sentiment(sentences)
word.count<-word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list<-rbind(sentence.list,
cbind(speech.list[i,-ncol(speech.list)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list<-
sentence.list%>%
filter(!is.na(word.count))
sentence.list.sel<-filter(sentence.list,
type=="inaug", Term==1)
sentence.list.sel$File<-factor(sentence.list.sel$File)
number_of_sentences<-tapply(sentence.list.sel$sentences, sentence.list.sel$President, length)# number of sentences
average_words_per_sentence<-round(tapply(sentence.list.sel$word.count, sentence.list.sel$President, mean),0) # average words per sentence
total_words<-tapply(sentence.list.sel$word.count, sentence.list.sel$President, sum)
count<-data.frame(number_of_sentences, average_words_per_sentence, total_words)
filenames <- Sys.glob("~/Desktop/Spr2017-Proj1-xuanzix/data/fulltext/*-1.txt")
texts <- filenames %>%
lapply(readLines) %>%
lapply(paste0, collapse = " ") %>%
lapply(as.String)
names(texts) <- substr(basename(filenames),6,nchar(basename(filenames))-4)
annotate_entities <- function(doc, annotation_pipeline) {
annotations <-  NLP::annotate(doc, annotation_pipeline)
AnnotatedPlainTextDocument(doc, annotations)
}
itinerants_pipeline <-list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator())
texts_annotated <- lapply(texts, annotate_entities, itinerants_pipeline)
#class(texts_annotated)
words<-lapply(texts_annotated, words)
#class(words)
count$vocabulary<-words%>%lapply(unique) %>%sapply(length)
count$total_words[3]<-2395;count$average_words_per_sentence[3]<-19
count$total_words[23]<-1366; count$average_words_per_sentence[23]<-24
count$total_words[28]<-2427; count$average_words_per_sentence[28]<-17
count$total_words[7]<-2459; count$average_words_per_sentence[7]<-18
colnames(count)<-c("Number of Sentences", "Average Words Per Sentence","Total Words","Total Vocabulary")
Average<-c(mean(count$`Number of Sentences`),mean(count$`Average Words Per Sentence`),mean(count$`Total Words`),mean(count$`Total Vocabulary`))
Average<-round(Average,0)
names(Average)<-colnames(count)
count<-rbind(Average,count)
rownames(count)[1]<-"average"
president.order<-c("average",rev(filter(speech.list, Term==1,type=="inaug")$President))
count1<-data.frame()
for(i in 1:40){
a<-count[rownames(count) == president.order[i],]
count1<-rbind(count1,a)
}
datatable(count1, options = list(pageLength = 10))
knitr::include_graphics(".../figs/1.png")
theme.widebar <-
theme(text = element_text(family = "Gill Sans", color = "#444444")) +
theme(plot.title = element_text(size = 20)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))+
theme(axis.text.y = element_text(size=10))+
theme(axis.title=element_text(size=12))
data1<-count1[-1,]
data1<-data1[39:1,]
data1$president<-rownames(data1)
data1$president <- factor(data1$president, levels = rev(president.order)[-40])
# Total words
p1<-data1%>%
ggplot(aes(x = president, y = data1$`Total Words`,group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(title = "Total Words") +
labs(x = "Presidents", y = "Total Words") +
theme.widebar
# Vocabulary
p2<-data1%>%
ggplot(aes(x = president, y = data1$`Total Vocabulary`,group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(title = "Total Vocabulary") +
labs(x = "Presidents", y = "Total Vocabulary") +
theme.widebar
grid.arrange(p1, p2, ncol = 2)
#Number of sentences
p3<-data1%>%
ggplot(aes(x = president, y = data1$`Number of Sentences`,group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(title = "Number of Sentences") +
labs(x = "Presidents", y = "Number of Sentences") +
theme.widebar
# Average Words Per Sentence
p4<-data1%>%
ggplot(aes(x = president, y = data1$`Average Words Per Sentence`,group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(title = "Average Words Per Sentence") +
labs(x = "Presidents", y = "Average Words Per Sentence") +
theme.widebar
grid.arrange(p3, p4, ncol = 2)
ff.all<-tm_map(ff.all, stripWhitespace)
folder.path="~/Desktop/Spr2017-Proj1-xuanzix/data/fulltext/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path,encoding="UTF-8"))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
dtm <- DocumentTermMatrix(ff.all,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
ff.dtm=tidy(dtm)
par(mfrow=c(2,2))
# Donald Trump
wordcloud(ff.dtm$term[ff.dtm$document==speeches[9]],
ff.dtm$count[ff.dtm$document==speeches[9]],
scale=c(3,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0.1,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Reds"),
main=prez.out[8])
#Barack Obama
wordcloud(ff.dtm$term[ff.dtm$document==speeches[6]],
ff.dtm$count[ff.dtm$document==speeches[6]],
scale=c(3,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0.1,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Reds"),
main=prez.out[6])
#George W Bush
wordcloud(ff.dtm$term[ff.dtm$document==speeches[19]],
ff.dtm$count[ff.dtm$document==speeches[19]],
scale=c(3,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0.1,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Reds"),
main=prez.out[19])
#WilliamJClinton
wordcloud(ff.dtm$term[ff.dtm$document==speeches[52]],
ff.dtm$count[ff.dtm$document==speeches[52]],
scale=c(3,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0.1,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Reds"),
main=prez.out[52])
count_america<-lapply(words, function(x){length(grep("America", x))})
names(count_america)<-rownames(count)[-1]
rev.president.order<-rev(president.order[-1])
count.america<-c()
for(i in 1:39){
a<-count_america[names(count_america) == rev.president.order[i]]
count.america<-c(count.america,a)
}
america<-data.frame(name=names(count.america), count=as.integer(count.america))
america$name <- factor(america$name, rev.president.order)
#Number of America
ggplot(america,aes(x = name, y = count, group=1)) +
geom_point(color="darkred")+
stat_summary(fun.y=sum,geom="line",color="darkred")+
labs(x = "Presidents", y = "Number of America") +
theme.widebar
f.plotsent.len=function(In.list, InFile, InType, InTerm, President){
col.use <- c("blue", "red")
In.list$topemotion=apply(select(In.list,
negative:positive),
1, which.max)
In.list$topemotion.v=apply(select(In.list,
negative:positive),
1, max)
temp <- In.list$topemotion.v
In.list$topemotion.v[In.list$topemotion==1] <- -temp
#In.list$topemotion.v[abs(In.list$topemotion.v)<0.05]=0
df <- In.list%>%filter(File==InFile,
type==InType, Term==InTerm)%>%
select(sent.id, topemotion, topemotion.v)
dct_values <- get_transformed_values(
df$topemotion.v,
low_pass_size = 5,
x_reverse_len = 100,
scale_vals = F,
scale_range = T
)
dct_signs <- 0.5*sign(dct_values)+1.5
ptcol.use <- col.use[dct_signs]
plot(dct_values,
col=ptcol.use,
lwd="2",
type="h",
main=President, ylab = "level", xlab = "Transcript")
abline(h=0,lwd="2")
}
getwd
getwd()
getwd()
getwd()
